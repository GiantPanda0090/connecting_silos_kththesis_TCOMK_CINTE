@Book{ref1,
author="{ABU, JONAS}
and {CATALAN, DIEGO}",
title="Prokrastinering i form av medieanv{\"a}ndning: En j{\"a}mf{\"o}relsestudie kring k{\"o}n och syssels{\"a}ttning",
abstract="A deep learning algorithm for improving the performance of the Sum-Product Algorithm (SPA) based decoders is investigated. The proposed Neural Network Decoders (NND) [22] generalizes the SPA by assigning weights to the edges of the Tanner graph. We elucidate the peculiar design, training, and working of the NND. We analyze the edge weight's distribution of the trained NND and provide a deeper insight into its working. The training process of NND learns the edge weights in such a way that the effects of artifacts in the Tanner graph (such as cycles or trapping sets) are mitigated, leading to a significant improvement in performance over the SPA. We conduct an extensive analysis of the training hyper-parameters affect- ing the performance of the NND, and present hypotheses for determining their appropriate choices for different families and sizes of codes. Experimental re- sults are used to verify the hypotheses and rationale presented. Furthermore, we propose a new loss-function that improves performance over the standard cross-entropy loss. We also investigate the limitations of the NND in terms of complexity and performance. Although the SPA based design of the NND enables faster training and reduced complexity, the design constraints restrict the neural network to reach its maximum potential. Our experiments show that the NND is unable to reach Maximum Likelihood (ML) performance threshold for any plausible set of hyper-parameters. However for short length (n ≤ 128) High Density Parity Check (HDPC) codes such as Polar or BCH codes, the performance improvement over the SPA is significant. ii"
}

